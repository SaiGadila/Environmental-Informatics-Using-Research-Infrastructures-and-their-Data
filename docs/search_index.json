[["flux-measurements-inter-operability.html", "Chapter 4 Flux Measurements &amp; Inter-Operability 4.1 Learning Objectives 4.2 Eddy Co_variance Data: What does it actually measure? 4.3 QA/QC Flags 4.4 Examples of Other Flux Networks: AMERIFLUX &amp; FLUXNET 4.5 The Power of Networked Ecology: Bridging to AMERIFLUX and Beyond 4.6 Hands On: Introduction to working with NEON eddy flux data 4.7 Exercises", " Chapter 4 Flux Measurements &amp; Inter-Operability Estimated Time: 3 hours Course participants: As you review this information, please consider the final course project that you will work on at the over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 4.1 Learning Objectives At the end of this activity, you will be able to: Define the eddy covariance method Articulate various carbon storage and flux terms Understand the structure of bundled eddy covariance data Be able to process NEON flux data such that it is comparable with AmeriFlux 4.2 Eddy Co_variance Data: What does it actually measure? 4.2.1 Example Eddy Site 4.3 QA/QC Flags 4.4 Examples of Other Flux Networks: AMERIFLUX &amp; FLUXNET AmeriFlux is a network of PI-managed sites measuring ecosystem CO2, water, and energy fluxes in North, Central and South America. It was established to connect research on field sites representing major climate and ecological biomes, including tundra, grasslands, savanna, crops, and conifer, deciduous, and tropical forests. As a grassroots, investigator-driven network, the AmeriFlux community has tailored instrumentation to suit each unique ecosystem. This “coalition of the willing” is diverse in its interests, use of technologies and collaborative approaches. As a result, the AmeriFlux Network continually pioneers new ground. The network was launched in 1996, after an international workshop on flux measurements in La Thuile, Italy, in 1995, where some of the first year-long flux measurements were presented. Early support for the network came from many sources, including the U.S. Department of Energy’s Terrestrial Carbon Program, the DOE’s National Institute of Global Environmental Change (NIGEC), NASA, NOAA and the US Forest Service. The network grew from about 15 sites in 1997 to more than 110 active sites registered today. Sixty-one other sites, now inactive, have flux data stored in the network’s database. In 2012, the U.S. DOE established the AmeriFlux Management Project (AMP) at Lawrence Berkeley National Laboratory (LBNL) to support the broad AmeriFlux community and the AmeriFlux sites. View the AMERIFLUX Network-at-a-Glance AmeriFlux is now one of the DOE Office of Biological and Environmental Research’s (BER) best-known and most highly regarded brands in climate and ecological research. AmeriFlux datasets, and the understanding derived from them, provide crucial linkages between terrestrial ecosystem processes and climate-relevant responses at landscape, regional, and continental scales. 4.5 The Power of Networked Ecology: Bridging to AMERIFLUX and Beyond Given that AmeriFlux has been collecting and coordinating eddy covariance data across the Americas since 1996. The network provides a common platform for data sharing and collaboration for organizations and individual private investigators collecting flux tower data. There are now &gt;470 registered flux tower sites in North, Central, and South America in the AmeriFlux network, many operated by individual researchers or universities. The towers collect eddy covariance data across a broad range of climate zones and ecosystem types, from Chile to Alaska and everywhere in between. Now, data from the NEON project is available through the AmeriFlux data portal. The NEON team has formatted data from the NEON flux towers to make it fully compatible with AmeriFlux data. This allows researchers to view, download and analyze data from the NEON flux towers alongside data from all of the other flux towers in the AmeriFlux network. With 47 flux towers at terrestrial field sites across the U.S., the NEON program is now the largest single contributor of flux tower data to the AmeriFlux network. NEON field sites are located in 20 ecoclimatic zones across the U.S., representing many distinct ecosystems. Eddy covariance data will be served using the same methods at each site for the entire 30-year life of the Observatory, allowing for unprecedented comparability across both time and space. 4.6 Hands On: Introduction to working with NEON eddy flux data 4.6.1 Setup Start by installing and loading packages and setting options. To work with the NEON flux data, we need the rhdf5 package, which is hosted on Bioconductor, and requires a different installation process than CRAN packages: install.packages(&#39;BiocManager&#39;) BiocManager::install(&#39;rhdf5&#39;) options(stringsAsFactors=F) library(neonUtilities) Use the zipsByProduct() function from the neonUtilities package to download flux data from two sites and two months. The transformations and functions below will work on any time range and site(s), but two sites and two months allows us to see all the available functionality while minimizing download size. Inputs to the zipsByProduct() function: dpID: DP4.00200.001, the bundled eddy covariance product package: basic (the expanded package is not covered in this tutorial) site: NIWO = Niwot Ridge and HARV = Harvard Forest startdate: 2018-06 (both dates are inclusive) enddate: 2018-07 (both dates are inclusive) savepath: modify this to something logical on your machine check.size: T if you want to see file size before downloading, otherwise F The download may take a while, especially if you’re on a slow network. zipsByProduct(dpID=&quot;DP4.00200.001&quot;, package=&quot;basic&quot;, site=c(&quot;NIWO&quot;, &quot;HARV&quot;), startdate=&quot;2018-06&quot;, enddate=&quot;2018-07&quot;, savepath=&quot;./data&quot;, check.size=F) ## Finding available files ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## ## Downloading files totaling approximately 345.260177 MB ## ./data/filesToStack00200 already exists. Download will proceed, but check for duplicate files. ## Downloading 4 files ## | | | 0% | |======================= | 33% ## NEON.D01.HARV.DP4.00200.001.2018-06.basic.20220120T173946Z.RELEASE-2022.zip could not be downloaded. Re-attempting. ## | |=============================================== | 67% ## NEON.D01.HARV.DP4.00200.001.2018-07.basic.20220120T173946Z.RELEASE-2022.zip could not be downloaded. Re-attempting. ## | |======================================================================| 100% ## NEON.D13.NIWO.DP4.00200.001.2018-07.basic.20220120T173946Z.RELEASE-2022.zip could not be downloaded. Re-attempting. ## ## 4 files successfully downloaded to ./data/filesToStack00200 4.6.2 Data Levels There are five levels of data contained in the eddy flux bundle. For full details, refer to the NEON algorithm document. Briefly, the data levels are: Level 0’ (dp0p): Calibrated raw observations Level 1 (dp01): Time-aggregated observations, e.g. 30-minute mean gas concentrations Level 2 (dp02): Time-interpolated data, e.g. rate of change of a gas concentration Level 3 (dp03): Spatially interpolated data, i.e. vertical profiles Level 4 (dp04): Fluxes The dp0p data are available in the expanded data package and are beyond the scope of this tutorial. The dp02 and dp03 data are used in storage calculations, and the dp04 data include both the storage and turbulent components. Since many users will want to focus on the net flux data, we’ll start there. 4.6.3 Extract Level 4 data (Fluxes!) To extract the Level 4 data from the HDF5 files and merge them into a single table, we’ll use the stackEddy() function from the neonUtilities package. stackEddy() requires two inputs: filepath: Path to a file or folder, which can be any one of: A zip file of eddy flux data downloaded from the NEON data portal A folder of eddy flux data downloaded by the zipsByProduct() function The folder of files resulting from unzipping either of 1 or 2 A single HDF5 file of NEON eddy flux data level: dp01-4 Input the filepath you downloaded to using zipsByProduct() earlier, including the filestoStack00200 folder created by the function, and dp04: flux &lt;- stackEddy(filepath=&quot;./data/filesToStack00200&quot;, level=&quot;dp04&quot;) We now have an object called flux. It’s a named list containing four tables: one table for each site’s data, and variables and objDesc tables. names(flux) Let’s look at the contents of one of the site data files: knitr::kable(head(flux$NIWO)) The variables and objDesc tables can help you interpret the column headers in the data table. The objDesc table contains definitions for many of the terms used in the eddy flux data product, but it isn’t complete. To get the terms of interest, we’ll break up the column headers into individual terms and look for them in the objDesc table: term &lt;- unlist(strsplit(names(flux$NIWO), split=&quot;.&quot;, fixed=T)) flux$objDesc[which(flux$objDesc$Object %in% term),] knitr::kable(term) For the terms that aren’t captured here, fluxCo2, fluxH2o, and fluxTemp are self-explanatory. The flux components are turb: Turbulent flux stor: Storage nsae: Net surface-atmosphere exchange The variables table contains the units for each field: knitr::kable(flux$variables) Let’s plot some data! First, we’ll need to convert the time stamps to an R date-time format (right now they’re just character fields). 4.6.4 Time stamps NEON sensor data come with time stamps for both the start and end of the averaging period. Depending on the analysis you’re doing, you may want to use one or the other; for general plotting, re-formatting, and transformations, I prefer to use the start time, because there are some small inconsistencies between data products in a few of the end time stamps. Note that all NEON data use UTC time, noted as tz=\"GMT\" in the code below. This is true across NEON’s instrumented, observational, and airborne measurements. When working with NEON data, it’s best to keep everything in UTC as much as possible, otherwise it’s very easy to end up with data in mismatched times, which can cause insidious and hard-to-detect problems. Be sure to include the tz argument in all the lines of code below - if there is no time zone specified, R will default to the local time zone it detects on your operating system. timeB &lt;- as.POSIXct(flux$NIWO$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%S&quot;, tz=&quot;GMT&quot;) flux$NIWO &lt;- cbind(timeB, flux$NIWO) plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, pch=&quot;.&quot;, xlab=&quot;Date&quot;, ylab=&quot;CO2 flux&quot;, xaxt=&quot;n&quot;) axis.POSIXct(1, x=timeB, format=&quot;%Y-%m-%d&quot;) Like a lot of flux data, these data have some stray spikes, but there is a clear diurnal pattern going into the growing season. Let’s trim down to just two days of data to see a few other details. plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, pch=20, xlab=&quot;Date&quot;, ylab=&quot;CO2 flux&quot;, xlim=c(as.POSIXct(&quot;2018-07-07&quot;, tz=&quot;GMT&quot;), as.POSIXct(&quot;2018-07-09&quot;, tz=&quot;GMT&quot;)), ylim=c(-20,20), xaxt=&quot;n&quot;) axis.POSIXct(1, x=timeB, format=&quot;%Y-%m-%d %H:%M:%S&quot;) Note the timing of C uptake; the UTC time zone is clear here, where uptake occurs at times that appear to be during the night. 4.6.5 Merge flux data with other sensor data Many of the data sets we would use to interpret and model flux data are measured as part of the NEON project, but are not present in the eddy flux data product bundle. In this section, we’ll download PAR data and merge them with the flux data; the steps taken here can be applied to any of the NEON instrumented (IS) data products. 4.6.5.1 Download PAR data To get NEON PAR data, use the loadByProduct() function from the neonUtilities package. loadByProduct() takes the same inputs as zipsByProduct(), but it loads the downloaded data directly into the current R environment. Let’s download PAR data matching the Niwot Ridge flux data. The inputs needed are: dpID: DP1.00024.001 site: NIWO startdate: 2018-06 enddate: 2018-07 package: basic avg: 30 The new input here is avg=30, which downloads only the 30-minute data. Since the flux data are at a 30-minute resolution, we can save on download time by disregarding the 1-minute data files (which are of course 30 times larger). The avg input can be left off if you want to download all available averaging intervals. pr &lt;- loadByProduct(&quot;DP1.00024.001&quot;, site=&quot;NIWO&quot;, avg=30, startdate=&quot;2018-06&quot;, enddate=&quot;2018-07&quot;, package=&quot;basic&quot;, check.size=F) pr is another named list, and again, metadata and units can be found in the variables table. The PARPAR_30min table contains a verticalPosition field. This field indicates the position on the tower, with 10 being the first tower level, and 20, 30, etc going up the tower. 4.6.5.2 Join PAR to flux data We’ll connect PAR data from the tower top to the flux data. pr.top &lt;- pr$PARPAR_30min[which(pr$PARPAR_30min$verticalPosition== max(pr$PARPAR_30min$verticalPosition)),] loadByProduct() automatically converts time stamps when it reads the data, so here we just need to indicate which time field to use to merge the flux and PAR data. timeB &lt;- pr.top$startDateTime pr.top &lt;- cbind(timeB, pr.top) And merge the two datasets: fx.pr &lt;- merge(pr.top, flux$NIWO, by=&quot;timeB&quot;) plot(fx.pr$data.fluxCo2.nsae.flux~fx.pr$PARMean, pch=&quot;.&quot;, ylim=c(-20,20), xlab=&quot;PAR&quot;, ylab=&quot;CO2 flux&quot;) If you’re interested in data in the eddy covariance bundle besides the net flux data, the rest of this tutorial will guide you through how to get those data out of the bundle. 4.6.6 Vertical profile data (Level 3) The Level 3 (dp03) data are the spatially interpolated profiles of the rates of change of CO2, H2O, and temperature. Extract the Level 3 data from the HDF5 file using stackEddy() with the same syntax as for the Level 4 data. prof &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp03&quot;) knitr::kable(head(prof$NIWO)) 4.6.7 Un-interpolated vertical profile data (Level 2) The Level 2 data are interpolated in time but not in space. They contain the rates of change at the measurement heights. Again, they can be extracted from the HDF5 files using stackEddy() with the same syntax: prof.l2 &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp02&quot;) knitr::kable(head(prof.l2$HARV)) Note that here, as in the PAR data, there is a verticalPosition field. It has the same meaning as in the PAR data, indicating the tower level of the measurement. 4.6.8 Calibrated raw data (Level 1) Level 1 (dp01) data are calibrated, and aggregated in time, but otherwise untransformed. Use Level 1 data for raw gas concentrations and atmospheric stable isotopes. Using stackEddy() to extract Level 1 data requires additional inputs. The Level 1 files are too large to simply pull out all the variables by default, and they include mutiple averaging intervals, which can’t be merged. So two additional inputs are needed: avg: The averaging interval to extract var: One or more variables to extract What variables are available, at what averaging intervals? Another function in the neonUtilities package, getVarsEddy(), returns a list of HDF5 file contents. It requires only one input, a filepath to a single NEON HDF5 file: vars &lt;- getVarsEddy(&quot;./data/filesToStack00200/NEON.D01.HARV.DP4.00200.001.nsae.2018-07.basic.h5&quot;) knitr::kable(head(vars)) Inputs to var can be any values from the name field in the table returned by getVarsEddy(). Let’s take a look at CO2 and H2O, 13C in CO2 and 18O in H2O, at 30-minute aggregation. Let’s look at Harvard Forest for these data, since deeper canopies generally have more interesting profiles: iso &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp01&quot;, var=c(&quot;rtioMoleDryCo2&quot;,&quot;rtioMoleDryH2o&quot;, &quot;dlta13CCo2&quot;,&quot;dlta18OH2o&quot;), avg=30) knitr::kable(head(iso$HARV)) Let’s plot vertical profiles of CO2 and 13C in CO2 on a single day. Here, for convenience, instead of converting the time stamps to a time format, it’s easy to use the character format to extract the ones we want using grep(). And discard the verticalPosition values that are string values - those are the calibration gases. iso.d &lt;- iso$HARV[grep(&quot;2018-06-25&quot;, iso$HARV$timeBgn, fixed=T),] iso.d &lt;- iso.d[-which(is.na(as.numeric(iso.d$verticalPosition))),] ggplot is well suited to these types of data, let’s use it to plot the profiles. library(ggplot2) g &lt;- ggplot(iso.d, aes(y=verticalPosition)) + geom_path(aes(x=data.co2Stor.rtioMoleDryCo2.mean, group=timeBgn, col=timeBgn)) + theme(legend.position=&quot;none&quot;) + xlab(&quot;CO2&quot;) + ylab(&quot;Tower level&quot;) g g &lt;- ggplot(iso.d, aes(y=verticalPosition)) + geom_path(aes(x=data.isoCo2.dlta13CCo2.mean, group=timeBgn, col=timeBgn)) + theme(legend.position=&quot;none&quot;) + xlab(&quot;d13C&quot;) + ylab(&quot;Tower level&quot;) g Warning message: “Removed 55 rows containing missing values (geom_path).” The legends are omitted for space, see if you can work out the times of day the different colors represent. 4.6.9 Convert NEON flux data variables to AmeriFlux FP standard Install and load packages #Install NEONprocIS.base from GitHub, this package is a dependency of eddy4R.base devtools::install_github(repo=&quot;NEONScience/NEON-IS-data-processing&quot;, ref=&quot;master&quot;, subdir=&quot;pack/NEONprocIS.base&quot;, dependencies=c(NA, TRUE)[2], repos=c(BiocManager::repositories(), # for dependencies on Bioconductor packages &quot;https://cran.rstudio.com/&quot;) # for CRAN ) #Install eddy4R.base from GitHub devtools::install_github(repo=&quot;NEONScience/eddy4R&quot;, ref=&quot;master&quot;, subdir=&quot;pack/eddy4R.base&quot;, dependencies=c(NA, TRUE)[2], repos=c(BiocManager::repositories(), # for dependencies on Bioconductor packages &quot;https://cran.rstudio.com/&quot;) # for CRAN ) packReq &lt;- c(&quot;rhdf5&quot;, &quot;eddy4R.base&quot;, &quot;jsonlite&quot;, &quot;lubridate&quot;) lapply(packReq, function(x) { print(x) if(require(x, character.only = TRUE) == FALSE) { install.packages(x) library(x, character.only = TRUE) }}) Select your site of interest from the list of NEON sites below. site &lt;- &quot;KONZ&quot; #&quot;BARR&quot;,&quot;CLBJ&quot;,&quot;MLBS&quot;,&quot;DSNY&quot;,&quot;NIWO&quot;,&quot;ORNL&quot;,&quot;OSBS&quot;, #&quot;SCBI&quot;,&quot;LENO&quot;,&quot;TALL&quot;,&quot;CPER&quot;,&quot;BART&quot;,&quot;HARV&quot;,&quot;BLAN&quot;, #&quot;SERC&quot;,&quot;JERC&quot;,&quot;GUAN&quot;,&quot;LAJA&quot;,&quot;STEI&quot;,&quot;TREE&quot;,&quot;UNDE&quot;, #&quot;KONA&quot;,&quot;KONZ&quot;,&quot;UKFS&quot;,&quot;GRSM&quot;,&quot;DELA&quot;,&quot;DCFS&quot;,&quot;NOGP&quot;, #&quot;WOOD&quot;,&quot;RMNP&quot;,&quot;OAES&quot;,&quot;YELL&quot;,&quot;MOAB&quot;,&quot;STER&quot;,&quot;JORN&quot;, #&quot;SRER&quot;,&quot;ONAQ&quot;,&quot;ABBY&quot;,&quot;WREF&quot;,&quot;SJER&quot;,&quot;SOAP&quot;,&quot;TEAK&quot;, #&quot;TOOL&quot;,&quot;BONA&quot;,&quot;DEJU&quot;,&quot;HEAL&quot;,&quot;PUUM&quot; } If you would like to download a set range of dates, define the following paramemters. If these are not defined, it will default to the entire record at the site #define start and end dates, optional, defaults to entire period of site operation. Use %Y-%m-%d format. dateBgn &lt;- &quot;2020-03-01&quot; dateEnd &lt;- &quot;2020-05-31&quot; # Data package from the portal Pack &lt;- c(&#39;basic&#39;,&#39;expanded&#39;)[1] #The version data for the FP standard conversion processing ver = paste0(&quot;v&quot;,format(Sys.time(), &quot;%Y%m%dT%H%m&quot;)) Specify Download directory for HDF5 files from the NEON data portal and output directory to save the resulting csv files. Change save paths to where you want the files on your computer. #download directory DirDnld=tempdir() #Output directory, change this to where you want to save the output csv DirOutBase &lt;-paste0(&quot;~/eddy/data/Ameriflux/&quot;,ver) Specify Data Product number, for the Bundled Eddy-Covariance files, this is DP4.00200.001 #DP number dpID &lt;- &#39;DP4.00200.001&#39; Get metadata from Ameriflux Site Info BADM sheets for the site of interest #Grab a list of all Ameriflux sites, containing site ID and site description sites_web &lt;- jsonlite::fromJSON(&quot;http://ameriflux-data.lbl.gov/AmeriFlux/SiteSearch.svc/SiteList/AmeriFlux&quot;) #Grab only NEON sites sitesNeon &lt;- sites_web[grep(pattern = paste0(&quot;NEON.*&quot;,site), x = sites_web$SITE_NAME),] #For all NEON sites siteNeon &lt;- sites_web[grep(pattern = paste0(&quot;NEON.*&quot;,site), x = sites_web$SITE_NAME),] metaSite &lt;- lapply(siteNeon$SITE_ID, function(x) { pathSite &lt;- paste0(&quot;http://ameriflux-data.lbl.gov/BADM/Anc/SiteInfo/&quot;,x) tmp &lt;- fromJSON(pathSite) return(tmp) }) Use Ameriflux site IDs to name metadata lists #use NEON ID as list name names(metaSite) &lt;- site #Use Ameriflux site ID as list name #names(metaSite) &lt;- sitesNeon$SITE_ID Check if dateBgn is defined, if not make it the initial operations date “IOCR” of the site if(!exists(&quot;dateBgn&quot;) || is.na(dateBgn) || is.null(dateBgn)){ dateBgn &lt;- as.Date(metaSite[[site]]$values$GRP_FLUX_MEASUREMENTS[[1]]$FLUX_MEASUREMENTS_DATE_START, &quot;%Y%m%d&quot;) } else { dateBgn &lt;- dateBgn }#End of checks for missing dateBgn #Check if dateEnd is defined, if not make it the system date if(!exists(&quot;dateEnd&quot;) || is.na(dateEnd) || is.null(dateEnd)){ dateEnd &lt;- as.Date(Sys.Date()) } else { dateEnd &lt;- dateEnd }#End of checks for missing dateEnd Grab the UTC time offset from the Ameriflux API timeOfstUtc &lt;- as.integer(metaSite[[site]]$values$GRP_UTC_OFFSET[[1]]$UTC_OFFSET) Create the date sequence setDate &lt;- seq(from = as.Date(dateBgn), to = as.Date(dateEnd), by = &quot;month&quot;) Start processing the site time range specified, verify that the site and date range are specified as intended msg &lt;- paste0(&quot;Starting Ameriflux FP standard conversion processing workflow for &quot;, site, &quot; for &quot;, dateBgn, &quot; to &quot;, dateEnd) print(msg) Create output directory by checking if the download directory exists and create it if not if(dir.exists(DirDnld) == FALSE) dir.create(DirDnld, recursive = TRUE) #Append the site to the base output directory DirOut &lt;- paste0(DirOutBase, &quot;/&quot;, siteNeon$SITE_ID) #Check if directory exists and create if not if(!dir.exists(DirOut)) dir.create(DirOut, recursive = TRUE) Download and extract data #Initialize data List dataList &lt;- list() #Read data from the API dataList &lt;- lapply(setDate, function(x) { # year &lt;- lubridate::year(x) # mnth &lt;- lubridate::month(x) date &lt;- stringr::str_extract(x, pattern = paste0(&quot;[0-9]{4}&quot;, &quot;-&quot;, &quot;[0-9]{2}&quot;)) tryCatch(neonUtilities::zipsByProduct(dpID = dpID, site = site, startdate = date, enddate = date, package = &quot;basic&quot;, savepath = DirDnld, check.size = FALSE), error=function(e) NULL) files &lt;- list.files(paste0(DirDnld, &quot;/filesToStack00200&quot;)) utils::unzip(paste0(DirDnld, &quot;/filesToStack00200/&quot;, files[grep(pattern = paste0(site,&quot;.*.&quot;, date, &quot;.*.zip&quot;), x = files)]), exdir = paste0(DirDnld, &quot;/filesToStack00200&quot;)) files &lt;- list.files(paste0(DirDnld, &quot;/filesToStack00200&quot;)) dataIdx &lt;- rhdf5::h5read(file = paste0(DirDnld, &quot;/filesToStack00200/&quot;, max(files[grep(pattern = paste0(site,&quot;.*.&quot;, date,&quot;.*.h5&quot;), x = files)])), name = paste0(site, &quot;/&quot;)) if(!is.null(dataIdx)){ dataIdx$dp0p &lt;- NULL dataIdx$dp02 &lt;- NULL dataIdx$dp03 &lt;- NULL dataIdx$dp01$ucrt &lt;- NULL dataIdx$dp04$ucrt &lt;- NULL dataIdx$dp01$data &lt;- lapply(dataIdx$dp01$data,FUN=function(var){ nameTmi &lt;- names(var) var &lt;- var[grepl(&#39;_30m&#39;,nameTmi)] return(var)}) dataIdx$dp01$qfqm &lt;- lapply(dataIdx$dp01$qfqm,FUN=function(var){ nameTmi &lt;- names(var) var &lt;- var[grepl(&#39;_30m&#39;,nameTmi)] return(var)}) } return(dataIdx) }) Add names to list for year/month combinations names(dataList) &lt;- paste0(lubridate::year(setDate),sprintf(&quot;%02d&quot;,lubridate::month(setDate))) Remove NULL elements from list dataList &lt;- dataList[vapply(dataList, Negate(is.null), NA)] Determine tower horizontal &amp; vertical indices #Find the tower top level by looking at the vertical index of the turbulent CO2 concentration measurements LvlTowr &lt;- grep(pattern = &quot;_30m&quot;, names(dataList[[1]]$dp01$data$co2Turb), value = TRUE) LvlTowr &lt;- gsub(x = LvlTowr, pattern = &quot;_30m&quot;, replacement = &quot;&quot;) #get tower top level LvlTop &lt;- strsplit(LvlTowr,&quot;&quot;) LvlTop &lt;- base::as.numeric(LvlTop[[1]][6]) #Ameriflux vertical levels based off of https://ameriflux.lbl.gov/data/aboutdata/data-variables/ section 3.3.1 &quot;Indices must be in order, starting with the highest.&quot; idxVerAmfx &lt;- base::seq(from = 1, to = LvlTop, by = 1) #get the sequence from top to first level LvlMeas &lt;- base::seq(from = LvlTop, to = 1, by = -1) #Recreate NEON naming conventions LvlMeas &lt;- paste0(&quot;000_0&quot;,LvlMeas,&quot;0&quot;,sep=&quot;&quot;) #Give NEON naming conventions to Ameriflux vertical levels names(idxVerAmfx) &lt;- LvlMeas #Ameriflux horizontal index idxHorAmfx &lt;- 1 Subset to the Ameriflux variables to convert dataListFlux &lt;- lapply(names(dataList), function(x) { data.frame( &quot;TIMESTAMP_START&quot; = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;, tz = &quot;GMT&quot;), &quot;TIMESTAMP_END&quot; = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeEnd, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;, tz = &quot;GMT&quot;), # &quot;TIMESTAMP_START&quot; = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;), format = &quot;%Y%m%d%H%M&quot;), # &quot;TIMESTAMP_END&quot; = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeEnd, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;) + 60, format = &quot;%Y%m%d%H%M&quot;), &quot;FC&quot;= dataList[[x]]$dp04$data$fluxCo2$turb$flux, &quot;SC&quot;= dataList[[x]]$dp04$data$fluxCo2$stor$flux, &quot;NEE&quot;= dataList[[x]]$dp04$data$fluxCo2$nsae$flux, &quot;LE&quot; = dataList[[x]]$dp04$data$fluxH2o$turb$flux, &quot;SLE&quot; = dataList[[x]]$dp04$data$fluxH2o$stor$flux, &quot;USTAR&quot; = dataList[[x]]$dp04$data$fluxMome$turb$veloFric, &quot;H&quot; = dataList[[x]]$dp04$data$fluxTemp$turb$flux, &quot;SH&quot; = dataList[[x]]$dp04$data$fluxTemp$stor$flux, &quot;FETCH_90&quot; = dataList[[x]]$dp04$data$foot$stat$distXaxs90, &quot;FETCH_MAX&quot; = dataList[[x]]$dp04$data$foot$stat$distXaxsMax, &quot;V_SIGMA&quot; = dataList[[x]]$dp04$data$foot$stat$veloYaxsHorSd, #&quot;W_SIGMA&quot; = dataList[[x]]$dp04$data$foot$stat$veloZaxsHorSd, &quot;CO2_1_1_1&quot; = dataList[[x]]$dp01$data$co2Turb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean, &quot;H2O_1_1_1&quot; = dataList[[x]]$dp01$data$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean, &quot;qfFinlH2oTurbFrt00Samp&quot; = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$frt00Samp$qfFinl, &quot;qfH2O_1_1_1&quot; = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl, &quot;qfCO2_1_1_1&quot; = dataList[[x]]$dp01$qfqm$co2Turb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl, &quot;qfSC&quot; = dataList[[x]]$dp04$qfqm$fluxCo2$stor$qfFinl, &quot;qfSLE&quot; = dataList[[x]]$dp04$qfqm$fluxH2o$stor$qfFinl, &quot;qfSH&quot; = dataList[[x]]$dp04$qfqm$fluxTemp$stor$qfFinl, &quot;qfT_SONIC&quot; = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$qfFinl, &quot;qfWS_1_1_1&quot; = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$qfFinl, rbind.data.frame(lapply(names(idxVerAmfx), function(y) { tryCatch({rlog$debug(y)}, error=function(cond){print(y)}) rpt &lt;- list() rpt[[paste0(&quot;CO2_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$data$co2Stor[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean rpt[[paste0(&quot;H2O_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$data$h2oStor[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean rpt[[paste0(&quot;CO2_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$data$isoCo2[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean rpt[[paste0(&quot;H2O_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$data$isoCo2[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean rpt[[paste0(&quot;qfCO2_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$co2Stor[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl rpt[[paste0(&quot;qfH2O_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$h2oStor[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl rpt[[paste0(&quot;qfCO2_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$isoCo2[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl rpt[[paste0(&quot;qfH2O_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$isoH2o[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl rpt &lt;- rbind.data.frame(rpt) return(rpt) } )), &quot;WS_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$mean, &quot;WS_MAX_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$max, &quot;WD_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$angZaxsErth$mean, &quot;T_SONIC&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$mean, &quot;T_SONIC_SIGMA&quot; = base::sqrt(dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$mean) , stringsAsFactors = FALSE) }) names(dataListFlux) &lt;- names(dataList) Combine the monthly data into a single dataframe, remove lists and clean memory dataDfFlux &lt;- do.call(rbind.data.frame,dataListFlux) rm(list=c(&quot;dataListFlux&quot;,&quot;dataList&quot;)) gc() Regularize timeseries to 30 minutes in case timestamps are missing from NEON files due to processing errors timeRglr &lt;- eddy4R.base::def.rglr(timeMeas = as.POSIXlt(dataDfFlux$TIMESTAMP_START), dataMeas = dataDfFlux, BgnRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_START[1]), EndRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_END[length(dataDfFlux$TIMESTAMP_END)]), TzRglr = &quot;UTC&quot;, FreqRglr = 1/(60*30)) #Reassign data to data.frame dataDfFlux &lt;- timeRglr$dataRglr #Format timestamps dataDfFlux$TIMESTAMP_START &lt;- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc), format = &quot;%Y%m%d%H%M&quot;) dataDfFlux$TIMESTAMP_END &lt;- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc) + lubridate::minutes(30), format = &quot;%Y%m%d%H%M&quot;) Define validation times, and remove this data from the dataset. At NEON sites, validations with a series of gasses of known concentration are run every 23.5 hours. These values are used to correct for measurment drift and are run every 23.5 hours to achive daily resolution while also spreading the impact of lost measurements throughout the day. #Remove co2Turb and h2oTurb data based off of qfFlow (qfFinl frt00) dataDfFlux$FC[(which(dataDfFlux$qfCO2_1_1_1 == 1))] &lt;- NaN dataDfFlux$LE[(which(dataDfFlux$qfH2O_1_1_1 == 1))] &lt;- NaN dataDfFlux$USTAR[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$H[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$SC[(which(dataDfFlux$qfSC == 1))] &lt;- NaN dataDfFlux$SLE[(which(dataDfFlux$qfSLE == 1))] &lt;- NaN dataDfFlux$SH[(which(dataDfFlux$qfSH == 1))] &lt;- NaN dataDfFlux$T_SONIC[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$T_SONIC_SIGMA[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$WS_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$WS_MAX_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$WD_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$H2O_1_1_1[(which(dataDfFlux$qfH2O_1_1_1 == 1))] &lt;- NaN dataDfFlux$CO2_1_1_1[(which(dataDfFlux$qfCO2_1_1_1 == 1))] &lt;- NaN lapply(idxVerAmfx, function(x){ #x &lt;- 1 dataDfFlux[[paste0(&quot;H2O_1_&quot;,x,&quot;_2&quot;)]][(which(dataDfFlux[[paste0(&quot;qfH2O_1_&quot;,x,&quot;_2&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;H2O_1_&quot;,x,&quot;_3&quot;)]][(which(dataDfFlux[[paste0(&quot;qfH2O_1_&quot;,x,&quot;_3&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;CO2_1_&quot;,x,&quot;_2&quot;)]][(which(dataDfFlux[[paste0(&quot;qfCO2_1_&quot;,x,&quot;_2&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;CO2_1_&quot;,x,&quot;_3&quot;)]][(which(dataDfFlux[[paste0(&quot;qfCO2_1_&quot;,x,&quot;_3&quot;)]] == 1))] &lt;&lt;- NaN }) Remove quality flagging variables from output setIdxQf &lt;- grep(&quot;qf&quot;, names(dataDfFlux)) dataDfFlux[,setIdxQf] &lt;- NULL Set range thresholds #assign list Rng &lt;- list() Rng$Min &lt;- data.frame( &quot;FC&quot; = -100, #[umol m-2 s-1] &quot;SC&quot; = -100, #[umol m-2 s-1] &quot;NEE&quot; = -100, #[umol m-2 s-1] &quot;LE&quot; = -500, #[W m-2] &quot;H&quot; = -500, #[W m-2] &quot;USTAR&quot; = 0, #[m s-1] &quot;CO2&quot; = 200, #[umol mol-1] &quot;H2O&quot; = 0, #[mmol mol-1] &quot;WS_1_1_1&quot; = 0, #[m s-1] &quot;WS_MAX_1_1_1&quot; = 0, #[m s-1] &quot;WD_1_1_1&quot; = -0.1, #[deg] &quot;T_SONIC&quot; = -55.0 #[C] ) Set Max thresholds Rng$Max &lt;- data.frame( &quot;FC&quot; = 100, #[umol m-2 s-1] &quot;SC&quot; = 100, #[umol m-2 s-1] &quot;NEE&quot; = 100, #[umol m-2 s-1] &quot;LE&quot; = 1000, #[W m-2] &quot;H&quot; = 1000, #[W m-2] &quot;USTAR&quot; = 5, #[m s-1] &quot;CO2&quot; = 800, #[umol mol-1] &quot;H2O&quot; = 100, #[mmol mol-1] &quot;WS_1_1_1&quot; = 50, #[m s-1] &quot;WS_MAX_1_1_1&quot; = 50, #[m s-1] &quot;WD_1_1_1&quot; = 360, #[deg] &quot;T_SONIC&quot; = 45.0 #[C] ) Grab all CO2/H2O columns to apply same thresholds, replace missing values with -9999 nameCO2 &lt;- grep(&quot;CO2&quot;,names(dataDfFlux),value = TRUE) nameH2O &lt;- grep(&quot;H2O&quot;,names(dataDfFlux),value = TRUE) #Apply the CO2/H2O threshold to all variables in HOR_VER_REP Rng$Min[nameCO2] &lt;- Rng$Min$CO2 Rng$Min[nameH2O] &lt;- Rng$Min$H2O Rng$Max[nameCO2] &lt;- Rng$Max$CO2 Rng$Max[nameH2O] &lt;- Rng$Max$H2O #Apply the range test to the output, and replace values with NaN lapply(names(dataDfFlux), function(x) { dataDfFlux[which(dataDfFlux[,x]&lt;Rng$Min[[x]] | dataDfFlux[,x]&gt;Rng$Max[[x]]),x] &lt;&lt;- NaN}) # Delete any NEE that have either FC or SC removed dataDfFlux[is.na(dataDfFlux$FC) | is.na(dataDfFlux$SC),&quot;NEE&quot;] &lt;- NaN #Change NA to -9999 dataDfFlux[is.na(dataDfFlux)] &lt;- -9999 Write output data to csv #Create output filename based off of Ameriflux file naming convention nameFileOut &lt;- base::paste0(DirOut,&quot;/&quot;,siteNeon$SITE_ID,&#39;_HH_&#39;,dataDfFlux$TIMESTAMP_START[1],&#39;_&#39;,utils::tail(dataDfFlux$TIMESTAMP_END,n=1),&#39;_flux.csv&#39;) #Write output to .csv write.csv(x = dataDfFlux, file = nameFileOut, row.names = FALSE) Clean up environment rm(list=&quot;dataDfFlux&quot;) gc() 4.7 Exercises 4.7.1 Computational NEON data are submitted to AmeriFlux quarterly after one year of non-quality flagged or otherwise missing data are available. Use the workflow above to extend the data coverage of an already submitted NEON site by downloading existing data from the AmeriFlux website and recently published HDF5 files from the NEON data portal. Process the NEON data such that it is in AmeriFlux format and plot the entire timerseries. Hint: NEON sites start with US-x Using metScanR package, find co-located NEON and AmeriFlux sites. Download data for an overlapping time period, and compare FC and H values by making a scatter plot and seeing how far off the data are from a 1:1 line. "]]
